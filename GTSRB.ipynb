{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l2/bg92b_lx0ys_slmmxyqqrtrh0000gn/T/ipykernel_5757/2980782279.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m '''\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "The GTSRB Dataset is a collection of German traffic signs. This programm is a CNN to detect and classify\n",
    "the signs.\n",
    "'''\n",
    "\n",
    "train_path = \"//Users/soeren/Documents/Neuronale Netze/Signs/GTSRB-2/Final_Training/Images\"\n",
    "test_path = \"/Users/soeren/Documents/Neuronale Netze/Signs/GTSRB/Final_Test/Images\"\n",
    "\n",
    "def list_all_elements_in_folder(folder):\n",
    "    elements = os.listdir(folder)\n",
    "    return elements\n",
    "    \n",
    "def all_elements_sorted_num_train(eles):\n",
    "    \"\"\"\n",
    "    Sorts the train_folder numerically for GTSB Project\n",
    "    \"\"\"\n",
    "    \n",
    "    fehler = 0\n",
    "    liste = []\n",
    "    for i in range(len(eles) - 1): #one .DS_Store file - unnecessary\n",
    "        liste.append(0)\n",
    "    for element in eles:\n",
    "        try:\n",
    "            liste.insert(int(element), element)\n",
    "            liste.pop(int(element) + 1)\n",
    "        except:\n",
    "            fehler += 1\n",
    "            \n",
    "    print(\"Fehler bei dem Laden: \", fehler)\n",
    "        \n",
    "    return liste\n",
    "        \n",
    "all_elements = all_elements_sorted_num_train(list_all_elements_in_folder(train_path))\n",
    "print(all_elements)\n",
    "print(len(os.listdir(train_path + '/' + all_elements[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection: Mapping dictionary to source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as img\n",
    "import numpy as np\n",
    "\n",
    "#Speicherung: training_dict{Klasse als String: Element}\n",
    "\n",
    "\n",
    "index = 0\n",
    "training_dict = {}\n",
    "valid_images = [\".ppm\"]\n",
    "while index < len(all_elements):\n",
    "    \n",
    "    new_path = train_path + '/' + all_elements[index]\n",
    "    klasse = str(index)\n",
    "    training_dict.update({klasse: []})\n",
    "    \n",
    "    for element in os.listdir(new_path):\n",
    "        if element[-3:] == \"ppm\":\n",
    "            \n",
    "            #ensures only training files (= files ending in .pppm are read)\n",
    "            \n",
    "            ext = os.path.splitext(element)[1]\n",
    "            if ext.lower() not in valid_images:\n",
    "                continue\n",
    "            pfad = os.path.join(new_path,element)\n",
    "            training_dict[klasse].append(pfad)\n",
    "            \n",
    "            \n",
    "    index += 1\n",
    "    \n",
    "example = training_dict[\"2\"]\n",
    "print(type(example))\n",
    "print(len(example))\n",
    "#print(example)\n",
    "print()\n",
    "#print(training_data['42'])\n",
    "print(len(training_dict))\n",
    "print(type(training_dict['0']))\n",
    "#training_data now contains all of the data in a dictionary, labelled by key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling and standardizing (-> reduce to gray tones) the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our labels\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def split_test_training(liste):\n",
    "    #trying to split in 70 % Training, 30% Test; Validation is in another folder, unlabelled\n",
    "    \n",
    "    anzahl_train = int(len(liste) / 10 * 7) #int always rounds down\n",
    "    \n",
    "    training = liste[:anzahl_train]\n",
    "    test = liste[anzahl_train:]\n",
    "    \n",
    "    return training, test\n",
    "    \n",
    "    \n",
    "\n",
    "labels = []\n",
    "for i in range(43):\n",
    "    labels.append(i)\n",
    "    \n",
    "#create the training data\n",
    "alle_daten = []\n",
    "index_klasse = 0\n",
    "resized = 50\n",
    "\n",
    "while index_klasse < len(training_dict):\n",
    "    index_klassenbilder = 0\n",
    "    \n",
    "    while index_klassenbilder < len(training_dict[str(index_klasse)]):\n",
    "        \n",
    "        img_array = cv2.imread(training_dict[str(index_klasse)][index_klassenbilder] ,cv2.IMREAD_GRAYSCALE) \n",
    "        new_array = cv2.resize(img_array, (resized, resized))\n",
    "        alle_daten.append([new_array, index_klasse]) \n",
    "        \n",
    "        \n",
    "        #Comments below for testing: should display one picture and then the same, resized to var resized x resized\n",
    "        \n",
    "        \n",
    "        #plt.imshow(img_array, cmap='gray')  # graph it\n",
    "        #plt.show()  # display!\n",
    "        #print(img_array.shape)\n",
    "        \n",
    "        #new_array = cv2.resize(img_array, (resized, resized))\n",
    "        #plt.imshow(new_array, cmap='gray')\n",
    "        #plt.show()\n",
    "        #print(new_array.shape)\n",
    "        \n",
    "        #break\n",
    "        \n",
    "        index_klassenbilder += 1\n",
    "        \n",
    "        \n",
    "    #break\n",
    "        \n",
    "    index_klasse += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(alle_daten)\n",
    "\n",
    "insgesamt = 0\n",
    "\n",
    "for element in alle_daten:\n",
    "    if element[1] == 2:\n",
    "        insgesamt += 1\n",
    "\n",
    "print(insgesamt)\n",
    "print(len(alle_daten))\n",
    "\n",
    "training_data, test_data = split_test_training(alle_daten)\n",
    "\n",
    "#for test purposes\n",
    "#traing_data: [[img,class],[img,class],...,[img,class]] is all randomnly shuffled\n",
    "\n",
    "for sample in training_data[:10]:\n",
    "    plt.imshow(sample[0])\n",
    "    plt.show()\n",
    "    print(sample[1])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Prep before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "\n",
    "for feature, label in training_data:\n",
    "    X_train.append(feature)\n",
    "    Y_train.append(label)\n",
    "    \n",
    "for feature, label in test_data:\n",
    "    X_test.append(feature)\n",
    "    Y_test.append(label)\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train = np.array(X_train).reshape(-1, resized, resized, 1)\n",
    "Y_train = to_categorical(Y_train)\n",
    "print(X_train.shape)\n",
    "\n",
    "X_test = np.array(X_test).reshape(-1, resized, resized, 1)\n",
    "Y_test = to_categorical(Y_test)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "plt.imshow(X_train[0].reshape(resized,resized),cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving our data to hard drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save everything, unnecessary in this case\n",
    "#But could use pickle to load data in new script\n",
    "#as the .pickle files are now in the Jupyter Projects folder\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_out = open(\"X_train.pickle\",\"wb\")\n",
    "pickle.dump(X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"Y_train.pickle\",\"wb\")\n",
    "pickle.dump(Y_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"X_test.pickle\",\"wb\")\n",
    "pickle.dump(X_test, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"Y_test.pickle\",\"wb\")\n",
    "pickle.dump(Y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading our data from hard drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_in = open(\"X_train.pickle\",\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"Y_train.pickle\",\"rb\")\n",
    "Y_train = pickle.load(pickle_in)\n",
    "\n",
    "X_train = X_train/255.0\n",
    "\n",
    "pickle_in = open(\"X_test.pickle\",\"rb\")\n",
    "X_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"Y_test.pickle\",\"rb\")\n",
    "Y_test = pickle.load(pickle_in)\n",
    "\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "my_batch_size = 4\n",
    "my_epochs = 10\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,kernel_size=(5,5), activation = 'relu', input_shape = (resized,resized,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64,kernel_size=(5,5),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(43, activation = 'softmax'))\n",
    "model.summary()\n",
    "print()\n",
    "print()\n",
    "\n",
    "model.compile(optimizer='Adam',metrics = ['accuracy'], loss = 'categorical_crossentropy')\n",
    "history = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size= my_batch_size,verbose = 1,epochs = my_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model.save(\"GTSRB_CNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reimporting the model so we don't have to execute all the code above (for the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model = models.load_model(\"GTSRB_CNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying the model with unrelated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_elements_sorted_num_test(eles):\n",
    "    \"\"\"\n",
    "    Sorts the train_folder for GTSB Project\n",
    "    \"\"\"\n",
    "    \n",
    "    fehler = 0\n",
    "    liste = []\n",
    "    for i in range(len(eles) - 1): #one .DS_Store file - unnecessary\n",
    "        liste.append(0)\n",
    "    for element in eles:\n",
    "        try:\n",
    "            liste.insert(int(element[:-4]), element) #sort digits until .ppm\n",
    "            liste.pop(int(element[:-4]) + 1)\n",
    "        except:\n",
    "            fehler += 1\n",
    "            \n",
    "    print(\"Fehler bei dem Laden: \", fehler)\n",
    "        \n",
    "    return liste\n",
    "\n",
    "all_test_elements = all_elements_sorted_num_test(list_all_elements_in_folder(test_path))\n",
    "#print(all_test_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary to clearly be able to compare result with picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dictionary = {}\n",
    "map_liste = ['20 limit', '30 limit', '50 limit', '60 limit', '70 limit', '80 limit', 'no limit 80','100 limit',\n",
    "            '120 limit', 'no overtaking, both cars', 'no overtaking left truck right car', 'right of way rocket',\n",
    "            'right of way yellow and white', 'give way red and white', 'stop sign', 'limited access red and white circle', \n",
    "            'limited access red and white circle for trucks', 'do not pass red circle white bar',\n",
    "            'attention triangle exclamation mark', 'attention triangle curve left', 'attention triangle curve right',\n",
    "            'windy road ahead triangle', 'bumps ahead triangle', 'slippery road ahead triangle', \n",
    "            'narrow road triangle', 'construction triangle', 'attention traffic light', 'attention pedestrians',\n",
    "            'attention child and parent running', 'attention bikes', 'attention snow', 'attention deer',\n",
    "            'no limit white circle two grey bars', 'must turn right blue circle', 'must turn left blue circle',\n",
    "            'must go straight blue circle', 'must go straight or right blue circle', 'must go straight or left blue circle',\n",
    "            'must turn into here right blue circle', 'must turn into here left blue circle', 'roundabout',\n",
    "            'no limit for no overtaking, both cars', 'no limit for no overtaking left truck right car']\n",
    "\n",
    "#print(len(map_liste))\n",
    "\n",
    "for i in range(len(map_liste)):\n",
    "    mapping_dictionary.update({i: map_liste[i]})\n",
    "\n",
    "#print(mapping_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "index = random.randint(0, len(all_test_elements))\n",
    "data = test_path + '/' + str(all_test_elements[index])\n",
    "\n",
    "def prediction(model, data):\n",
    "    img_array = cv2.imread(data ,cv2.IMREAD_GRAYSCALE) \n",
    "    img_array = cv2.resize(img_array, (50, 50))\n",
    "    img_array = np.array(img_array).reshape(-1, 50, 50, 1)\n",
    "    predict = model.predict(img_array)\n",
    "    plt.imshow(img_array.reshape(50,50))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    #print(predict)\n",
    "    stelle = 0\n",
    "    for i in np.nditer(predict):\n",
    "        if i == 0:\n",
    "            stelle += 1\n",
    "        if i == 1:\n",
    "            break\n",
    "    return mapping_dictionary[stelle]\n",
    "\n",
    "prediction(model,data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
